{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7015aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. ... .... .. ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1eec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.variable import Variable\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e857ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "latent_dim = 100\n",
    "input_dim = 3 * 32 * 32\n",
    "output_dim = input_dim\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "sample_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18f28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3173ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a072eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413753a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54cf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "generator = Generator(latent_dim, output_dim).to(device)\n",
    "discriminator = Discriminator(input_dim).to(device)\n",
    "autoencoder = Autoencoder(input_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caef6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "autoencoder_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d11d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9be6440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|██▎  | 9/20 [01:23<01:42,  9.35s/batch, Discriminator Loss=0.241]Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training:  95%|███▊| 19/20 [02:56<00:09,  9.27s/batch, Discriminator Loss=0.201]Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training: 100%|███| 20/20 [03:06<00:00,  9.34s/batch, Discriminator Loss=0.0382]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total = num_epochs, desc='Training', unit='batch')\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_dataloader):\n",
    "        # Adversarial ground truths\n",
    "        real_labels = torch.ones((real_images.size(0), 1)).to(device)\n",
    "        fake_labels = torch.zeros((batch_size, 1)).to(device)\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_images = real_images.view(-1, input_dim).to(device)\n",
    "        real_outputs = discriminator(real_images)\n",
    "        real_loss = adversarial_loss(real_outputs, real_labels)\n",
    "\n",
    "        sample_latent = prepare_sample_dataset(batch_size)\n",
    "        fake_images = generator(sample_latent)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        fake_loss = adversarial_loss(fake_outputs, fake_labels)\n",
    "\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        generator_labels = torch.ones_like(fake_outputs)  # Generate labels for the generator\n",
    "\n",
    "        generator_loss = adversarial_loss(fake_outputs, generator_labels)\n",
    "        generator_loss.backward(retain_graph=True)  # Set retain_graph=True\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "    \n",
    "\n",
    "    # Sample and save images\n",
    "    if (epoch + 1) % sample_interval == 0:\n",
    "        sample_latent = prepare_sample_dataset(25)\n",
    "        generated_images = generator(sample_latent).detach()\n",
    "        generated_images = generated_images.view(-1, 3, 32, 32).cpu().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(generated_images[i].transpose((1, 2, 0)), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.savefig(f\"generated_images_epoch_{epoch + 1}.png\")\n",
    "        plt.close()\n",
    "    pbar.set_postfix({'Discriminator Loss': discriminator_loss.item()})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddc7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
